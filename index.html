
<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Jason Lee</title>

    <meta name="author" content="Jason Lee">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/globe.svg" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script>
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Jason Lee
                </p>
                <p>
			I am a incoming master student at UW ECE. My research interests..
                </p>
    
		      
                <p style="text-align:center">
                  <a href="mailto:jason328@uw.edu">Email</a> &nbsp;/&nbsp;
                  
                  <a href="https://github.com/jasonlee328/">Github</a> &nbsp;/&nbsp;
                  
                </p>
              </td>
    
            </tr>
          </tbody></table>
<!-- 

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <!-- <p>
                  Coming soon.
                </p> -->
              </td>
            </tr>
          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='nuvo_image'><video  width=115% muted autoplay loop>
                  <source src="images/MDN_anim.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/MDN_teaser.jpg' width=100%>
<!-- 		  <img src='images/mdn_arch.jpg' width=100%> -->
                </div>
                <script type="text/javascript">
                  function nuvo_start() {
                    document.getElementById('nuvo_image').style.opacity = "1";
                  }
        
                  function nuvo_stop() {
                    document.getElementById('nuvo_image').style.opacity = "0";
                  }

                  nuvo_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://hee-jae-kim.github.io/">
                  <span class="papertitle">Motion Diversification Networks</span>
                </a>
                <br>
                <strong>Hee Jae Kim</strong>,
                <a href="https://eshed1.github.io/">Eshed Ohn-Bar</a>
                <br>
                <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2024
                <br>
                <a href="https://mdncvpr.github.io/">project page</a>
                /
                <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_Motion_Diversification_Networks_CVPR_2024_paper.pdf">paper</a>
		/
		<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_Motion_Diversification_Networks_CVPR_2024_paper.pdf">code</a>
		/
		<a href="files/MDN_cvpr24_poster.pdf">poster</a>
                <p></p>
                <p>
                  We introduce Motion Diversification Networks, a novel framework for learning to generate realistic and diverse 3D human motion. Towards more realistic and functional 3D motion models, this work uncovers limitations in existing generative modeling techniques, particularly in overly simplistic latent code sampling strategies.
                </p>
              </td>
            </tr>
          </tbody></table>


	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/rainbirdgeo.jpeg' width=100%>
                </div>
                <script type="text/javascript">
                  function nuvo_start() {
                    document.getElementById('nuvo_image').style.opacity = "1";
                  }
        
                  function nuvo_stop() {
                    document.getElementById('nuvo_image').style.opacity = "0";
                  }
                  nuvo_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://journals.ametsoc.org/view/journals/apme/62/8/JAMC-D-22-0175.1.xml">
                  <span class="papertitle">Unsupervised Clustering of Geostationary Satellite Cloud Properties for Estimating Precipitation Probabilities of Tropical Convective Clouds</span>
                </a>
                <br>
		Doyi Kim,
                <strong>Hee Jae Kim</strong>,
		Yong-Sang Choi
                <br>
                <em>Journal of Applied Meteorology and Climatology (JAMC)</em>, 2023
                <br>
                <a href="https://journals.ametsoc.org/view/journals/apme/62/8/JAMC-D-22-0175.1.xml">paper</a>
                <p></p>
                <p>This study aims to explore the cloud properties of tropical convective clouds (TCC) that indicate a high probability of precipitation by training a neural network with daytime satellite imagery. </p>
              </td>
            </tr>
          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/360SR.jpg' width=100%>
                </div>
                <script type="text/javascript">
                  function nuvo_start() {
                    document.getElementById('nuvo_image').style.opacity = "1";
                  }
        
                  function nuvo_stop() {
                    document.getElementById('nuvo_image').style.opacity = "0";
                  }
                  nuvo_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9617634">
                  <span class="papertitle">360&deg; Image Reference-Based Super-Resolution using Latitude-Aware Convolution Learned from Synthetic to Real</span>
                </a>
                <br>
                <strong>Hee Jae Kim</strong>,
		<a href="https://sagittak.wixsite.com/icplab">Jewon Kang</a>,
		<a href="https://home.ewha.ac.kr/~bulee/">Byung-Uk Lee</a>
                <br>
                <em>IEEE Access</em>, 2021
                <br>
                <a href="https://iamheejae.github.io/lat360.github.io/">project page</a>
                /
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9617634">paper</a>
		/ 
		<a href="https://github.com/iamheejae/Lat360">code</a>
                <p></p>
                <p>We propose an efficient reference-based 360&deg; image super-resolution (RefSR) technique to exploit a wide field of view (FoV) among adjacent 360&deg; cameras. We do not assume any structured camera arrays but use a reference image captured in an arbitrary viewpoint. Accordingly, we develop a long-range 360 disparity estimator (DE360) to overcome a large and distorted disparity between equirectangular projection (ERP) images, particularly near the poles. </p>
              </td>
            </tr>
          </tbody></table> -->


        </td>
      </tr>
    </tbody></table>
  </body>
</html>
